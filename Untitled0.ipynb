{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Telliliskander/tug/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i_sJU2f9Xf0G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM,  BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
        "from tensorflow.keras import layers, callbacks\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "\n",
        "CC_A_PRED = \"LTC-USD\"\n",
        "PERIODE_PRED = 3  # peride de prediction en minutes\n",
        "LONG_SEQ = 60  # longueur de la sequence \n",
        "TAUX_VAL=0.05\n",
        "EPOCHS = 10  \n",
        "BATCH_SIZE = 64 \n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    min_delta=0.001, \n",
        "    patience=20, \n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess_df(df):    \n",
        "    df = df.drop(\"futur\", 1)\n",
        "    for col in df.columns:  \n",
        "        if col != \"decision\":  \n",
        "            df[col] = df[col].pct_change()  \n",
        "            df.dropna(inplace=True)  # supprimer les 'NaN' crées par pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # echelonner les donnes (entre 0 et 1)\n",
        "\n",
        "    df.dropna(inplace=True)  \n",
        "\n",
        "\n",
        "    seq = []  # listes des sequences\n",
        "    heure = deque(maxlen=LONG_SEQ)  # file\n",
        "    for i in df.values:  # df.values est un ndarray de dimensions 2 dont les lignes = les minutes et les colonnes = les caracteristiques\n",
        "        heure.append([n for n in i[:-1]])  # i est ndarray a 1 dimension (minute)\n",
        "        if len(heure) == LONG_SEQ:  \n",
        "            seq.append([np.array(heure), i[-1]])  # i[-1] = decision de la derniere minute dans l'heure par rapport aux 3 min precedentes\n",
        "    random.shuffle(seq)  # melanger des les sequences (les heures) au hasard\n",
        "\n",
        "    achat = []  \n",
        "    vente = []  \n",
        "\n",
        "    for sequence, dec in seq:  # (sequence,dec) est une heure dont sequence est un ndarray a 2 dimensions \n",
        "        if dec == 0:  \n",
        "            vente.append([sequence, dec])  \n",
        "        elif dec == 1:  \n",
        "            achat.append([sequence, dec])  \n",
        "\n",
        "    random.shuffle(achat)  \n",
        "    random.shuffle(vente)  \n",
        "\n",
        "    lower = min(len(achat), len(vente))  \n",
        "\n",
        "    achat = achat[:lower]  \n",
        "    vente = vente[:lower]  \n",
        "\n",
        "    seq2 = achat+vente  \n",
        "    random.shuffle(seq2)  \n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for sequence,dec in seq2:  \n",
        "        X.append(sequence) \n",
        "        y.append(dec)  \n",
        "\n",
        "    return np.array(X), np.array(y) \n",
        "\n",
        "\n",
        "\n",
        "def classifier(actuel, futur):\n",
        "    if float(futur) > float(actuel): \n",
        "        return 1\n",
        "    else:  \n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame()\n",
        "\n",
        "ccs = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # les 4 cc \n",
        "for cc in ccs:  \n",
        "\n",
        "    print(cc)\n",
        "    dataset = f'/content/{cc}.csv'  \n",
        "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  \n",
        "\n",
        "    df.rename(columns={\"close\": f\"{cc}_close\", \"volume\": f\"{cc}_volume\"}, inplace=True)\n",
        "    df.set_index(\"time\", inplace=True) # prendre time comme indice\n",
        "    df = df[[f\"{cc}_close\", f\"{cc}_volume\"]]  # ignorer les autres colonnes autre que close et volume\n",
        "\n",
        "    if len(main_df)==0:  \n",
        "        main_df = df  \n",
        "    else:  \n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # remplir les troux dans les données avec les valeurs precedentes\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  \n",
        "\n",
        "main_df['futur'] = main_df[f'{CC_A_PRED}_close'].shift(-PERIODE_PRED)\n",
        "main_df['decision'] = list(map(classifier, main_df[f'{CC_A_PRED}_close'], main_df['futur']))\n",
        "\n",
        "main_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "temps = sorted(main_df.index.values)\n",
        "dern_pct = temps[-int(TAUX_VAL*len(temps))] # l'indice a partir du quel on va separer les données dentrainement et celles de validation\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= dern_pct)]\n",
        "main_df = main_df[(main_df.index < dern_pct)]\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y),\n",
        "    #callbacks=[early_stopping],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JK4nHN0MK9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p66_tz5oYcRY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "# Set Matplotlib defaults\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "plt.rc('animation', html='html5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hcnlz24VVZl"
      },
      "outputs": [],
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "print(history_df)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
        "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfD9Oqvnd3wA+8YEZ40o8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}